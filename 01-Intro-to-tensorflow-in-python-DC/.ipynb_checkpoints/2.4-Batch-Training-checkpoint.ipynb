{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f983ff-5b6b-4f6f-afc1-19179baac71c",
   "metadata": {},
   "source": [
    "# Tensorflow Developer Certificate Preparation\n",
    "___\n",
    "## Introduction to Tensorflow in Python - DataCamp - ML-Scientist-Career-Track - by Isaiah Hull\n",
    "___\n",
    "## Chapter 2.4 - Batch Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eecc27-8cb1-4f9c-a9dd-2bb9585a27b3",
   "metadata": {},
   "source": [
    "### 1. Batch training\n",
    "Earlier in the chapter, we learned how to train a linear model to predict house prices. In this video, we will use batch training to handle large datasets.\n",
    "\n",
    "### 2. What is batch training?\n",
    "To answer this, let's return to the linear model you used to predict house prices earlier in the chapter. \n",
    "- But this time, let's say the dataset is much larger and you want to perform the training on a GPU, which has only small amount of memory. \n",
    "- Since you can't fit the entire dataset in memory, you will instead divide it into batches and train on those batches sequentially. \n",
    "- A single pass over all of the batches is called an **epoch** and the process itself is called **batch training**. \n",
    "- It will be quite useful when you work with large image datasets. \n",
    "- Beyond alleviating memory constraints, batch training will also allow you to update model weights and optimizer parameters after each batch, rather than at the end of the epoch.\n",
    "\n",
    "### 3. The chunksize parameter\n",
    "Earlier, we discussed using pandas to load data with read csv. \n",
    "- The same function can be used to load data in batches. \n",
    "- If, for instance, we have a 100 gigabyte dataset, we might want to avoid loading it all at once. \n",
    "- We can do this by using the chunksize parameter. \n",
    "- The code block shows how this can be done. \n",
    "- Let's first import pandas and numpy. \n",
    "- Instead of loading the data in a single one-liner, we'll write a for loop that iterates through the data in steps of 100 examples. \n",
    "- Each 100 will be available as batch, which we can use to extract columns, such as price and size in the housing dataset. \n",
    "- We can then convert these to numpy arrays and use them to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab68986-b1c1-4500-9781-4095ed68dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load data in batches\n",
    "for batch in pd.read_csv('./datasets/kc_house_data.csv', chunksize = 100):\n",
    "    # Extract price column\n",
    "    price = np.array(batch['price'], np.float32)\n",
    "    \n",
    "    # Extract size column\n",
    "    size = np.array(batch['sqft_living'], np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e282fa-26ba-467d-8e50-8263c6dd65d5",
   "metadata": {},
   "source": [
    "### 4. Training a linear model in batches\n",
    "- We now know how to load data from csv files in fixed-size batches using pandas. \n",
    "- This means that we can handle data sets of tens or even hundreds of gigabytes without exceeding the memory constraints of our system. \n",
    "- Let's look at a minimal example with a linear model using the King County housing dataset. \n",
    "- We will start by loading tensorflow, pandas, and numpy. \n",
    "- Next, we'll define variables for the intercept and slope, along with the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663d7596-2781-47a7-91a5-c511aae185ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow, pandas, and numpy\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# define trainable variables\n",
    "intercept = tf.Variable(0.1, tf.float32)\n",
    "slope = tf.Variable(0.1, tf.float32)\n",
    "\n",
    "# Define a linear regression model\n",
    "def linear_regression(intercept, slope = slope, features = size):\n",
    "        return intercept + features*slope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e57fa-909a-4437-bc90-f5d77e0d4f42",
   "metadata": {},
   "source": [
    "### 5. Training a linear model in batches\n",
    "- We then define a loss function, which takes the slope and intercept, and two sources of data: the ``features`` and the ``targets``. \n",
    "- It then returns the ``mean squared error loss``. \n",
    "- After defining the loss function, we instantiate an adam optimizer, which we will use to perform minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c68a8a02-302a-47b9-b480-9fd4841ac7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a loss function to compute the MSE\n",
    "def loss_function(intercept, slope, targets = price, features = size):\n",
    "    # compute the predictions for a linear model\n",
    "    predictions = linear_regression(intercept, slope)\n",
    "    \n",
    "    # Return the loss\n",
    "    return tf.keras.losses.mse(targets, predictions)\n",
    "\n",
    "# Define an optimization operation\n",
    "opt = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4f06a-8a79-45fd-8a37-993505ef3ff5",
   "metadata": {},
   "source": [
    "### 6. Training a linear model in batches\n",
    "The next step is to train the model in batches. \n",
    "- Again, we do this by using a for loop and supplying a chunksize to the read csv function. \n",
    "- Note that we take each batch, separate it into features and a target, convert those into numpy arrays, and then pass them to the minimize operation. \n",
    "- Within the minimize operation, we pass the loss function as a lambda function and we supply a variable list that contains only the trainable parameters, intercept and slope. \n",
    "- This loop will continue until we have stepped through all of the examples in csv read. Importantly, we did not ever need to have more than 100 examples in memory during the entire process. \n",
    "- Finally, we print our trained intercept and slope. \n",
    "- Note that we did not use default argument values for input data. \n",
    "- This is because our input data was generated in batches during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f37ffc5c-98d8-490e-8b96-8dd4444ecfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data in batches\n",
    "for batch in pd.read_csv('./datasets/kc_house_data.csv', chunksize = 100):\n",
    "    # Extract the target and feature columns\n",
    "    price_batch = np.array(batch['price'], np.float32)\n",
    "    size_batch = np.array(batch['sqft_lot'], np.float32)\n",
    "    \n",
    "    # Minimize the loss function\n",
    "    opt.minimize(lambda: loss_function(intercept, slope),\\\n",
    "                            var_list = [intercept, slope])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f660b905-fe97-4b04-aed9-8a0efb0fee6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31696486 0.3169659\n"
     ]
    }
   ],
   "source": [
    "# print the trained parameters\n",
    "print(intercept.numpy(), slope.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bee295-59e3-4078-9a6a-a619d7a6879e",
   "metadata": {},
   "source": [
    "### 7. Full sample versus batch training\n",
    "So what is the value of batch training? When we trained with the full sample, we updated the optimizer and model parameters once per training epoch and passed data to the loss function without modification, but were limited by memory constraints. With batch training, we updated the model weights and optimizer parameters multiple times each epoch and divided the data into batches, but no longer faced any memory constraints. In later chapters, you'll automate batch training by using high level APIs. Importantly, however, high level APIs will not typically load the sample in batches by default, as we have done here.\n",
    "![2.4.1](./figures/2.4.1.PNG)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
