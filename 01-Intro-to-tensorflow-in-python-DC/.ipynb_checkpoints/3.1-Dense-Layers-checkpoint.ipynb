{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a9a626-bc53-4805-8fe9-637ce99ea875",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tensorflow Developer Certificate Preparation\n",
    "___\n",
    "## Introduction to Tensorflow in Python - DataCamp - ML-Scientist-Career-Track - by Isaiah Hull\n",
    "___\n",
    "## Chapter 3.1 - Dense Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988b7218-a679-4fe7-a0e4-875605b00d27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1. Dense layers\n",
    "In this chapter, we will focus on training neural networks in TensorFlow. We will start with an overview of a frequently used component of neural networks: the dense layer.\n",
    "\n",
    "### 2. The linear regression model\n",
    "Throughout this chapter, we'll make use of a dataset on credit card default. It contains features, such as marital status and payment amount, which we'll use to predict a target, default. Here, we have the familiar linear regression model. We take marital status, which is 1, and bill amount, which is 3. We then multiply the inputs by their respective weights, zero point one and minus zero point two five, and sum.  \n",
    "![3.1.1](./figures/3.1.1.PNG)\n",
    "\n",
    "### 3. What is a neural network?\n",
    "So how do we get from a linear regression to a neural network? By adding a hidden layer, which, in this case, consists of two nodes. Each hidden layer node takes our two inputs, multiplies them by their respective weights, and sums them together. We also typically pass the hidden layer output to an activation function, but we will come back to that later. Finally, we sum together the outputs of the two hidden layers to compute our prediction for default. This entire process of generating a prediction is referred to as forward propagation.  \n",
    "![3.1.2](./figures/3.1.2.PNG)\n",
    "\n",
    "In this chapter, we will construct neural networks with three types of layers: an input layer, some number of hidden layers, and an output layer. The input layer consists of our features. The output layer contains our prediction. Each hidden layer takes inputs from the previous layer, applies numerical weights to them, sums them together, and then applies an activation function. In the neural network graph, we have applied a particular type of hidden layer called a dense layer. A dense layer applies weights to all nodes from the previous layer. We will use dense layers throughout this chapter to construct networks.  \n",
    "![3.1.3](./figures/3.1.3.PNG)\n",
    "\n",
    "### 4. A simple dense layer\n",
    "Let's look at a simple example of a dense layer. \n",
    "- We'll first define a constant tensor that contains the marital status and age data as the input layer. \n",
    "- We then initialize weights as a variable, since we will train those weights to predict the output from the inputs. \n",
    "- We also define a bias, which will play a similar role to the intercept in the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eb3716b-d56d-442e-bdbc-4492415c3033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define inputs (features)\n",
    "inputs = tf.constant([[1,35]], tf.float32)\n",
    "\n",
    "# define weights\n",
    "weights = tf.Variable([[-0.05], [-0.01]])\n",
    "\n",
    "# define a bias\n",
    "bias = tf.Variable([0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66cbf6a-7244-454f-a8fe-bb6f46e6ee50",
   "metadata": {},
   "source": [
    "- Finally, we define a dense layer. \n",
    "- Note that we first perform a matrix multiplication of the inputs by the weights and assign that to the tensor named product. \n",
    "- We then add product to the bias and apply a non-linear transformation, in this case the sigmoid function. \n",
    "- This is called the activation function and we will explore this in more depth in the next video, but do not worry about it for now. \n",
    "- Furthermore, note that the bias is not associated with a feature and is analogous to the intercept in a linear regression. \n",
    "- We will typically not draw it in neural network diagrams for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "240eac9b-15be-40e9-b069-2734dcffd0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4a494a3-b079-47fa-823c-0d4f4e71b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply inputs (features) by the weights\n",
    "product = tf.matmul(inputs, weights)\n",
    "\n",
    "# define dense layer\n",
    "dense = tf.keras.activations.sigmoid(product+bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77afeed4-f534-4de7-b509-f2175471457f",
   "metadata": {},
   "source": [
    "![3.1.4](./figures/3.1.4.PNG)\n",
    "\n",
    "### 5. Defining a complete model\n",
    "Note that TensorFlow also comes with higher level operations, such as tf dot keras dot layers dot Dense, which allows us to skip the linear algebra. \n",
    "- In this example, we take input data and convert it to a 32-bit float tensor. \n",
    "- We then define a first hidden dense layer using keras layers dense. \n",
    "- The first argument specifies the number of outgoing nodes. \n",
    "- And the second argument is the activation function. \n",
    "- By default, a bias will be included. \n",
    "- Note that we've also passed inputs as an argument to the first dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347cbbe6-8049-401b-8d0b-fc2932b7ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # define input (features) layer\n",
    "# inputs = tf.constant(data, tf.float32)\n",
    "\n",
    "# # define first dense layer\n",
    "# dense1 = tf.keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "\n",
    "# # define second dense layer\n",
    "# dense2 = tf.keras.layers.Dense(5, activation='sigmoid')(dense1)\n",
    "\n",
    "# # define output (predictions) layer\n",
    "# output = tf.keras.layers.Dense(1, activation='sigmoid')(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d1bdc-bc58-4a07-976b-0e14bd7ab113",
   "metadata": {},
   "source": [
    "- We can easily define another dense layer, which takes the first dense layer as an argument and then reduces the number of nodes. \n",
    "- The output layer reduces this again to one node.  \n",
    "![3.1.5](./figures/3.1.5.PNG)\n",
    "\n",
    "### 6. High-level versus low-level approach\n",
    "Finally, let's compare the high-level and low-level approaches. \n",
    "- The high-level approach relies on complex operations in high-level APIs, such as Keras and Estimators, reducing the amount of code needed. \n",
    "- The weights and the mathematical operations will typically be hidden by the layer constructor. \n",
    "- The low-level approach uses linear algebra, which allows for the construction of any model. \n",
    "- TensorFlow allows us to use either approach or even combine them.  \n",
    "![3.1.6](./figures/3.1.6.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72055550-67ea-4588-921d-0e56ecc433e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
